import streamlit as st
import os
from typing import TypedDict, List, Dict
from pprint import pformat

from langgraph.graph import StateGraph, END
from langchain_community.document_loaders import WebBaseLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from tavily import TavilyClient


# --- 1. ìƒíƒœ ì •ì˜ (State Definition) ---
class GraphState(TypedDict):
    question: str
    documents: List[Document]
    generation: str
    is_relevant: bool
    is_hallucination: bool
    search_attempts: int
    regeneration_attempts: int
    error: str
    final_answer: str
    final_sources: List[Dict]


# --- 2. ì›Œí¬í”Œë¡œìš° êµ¬ì„± í•¨ìˆ˜ ---
# Streamlit ì•±ì—ì„œ API í‚¤ë¥¼ ë°›ì€ í›„ ì›Œí¬í”Œë¡œìš°ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ê¸° ìœ„í•´ í•¨ìˆ˜ë¡œ ë¬¶ìŠµë‹ˆë‹¤.
def create_workflow(llm, retriever, tavily_client):
    """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤."""

    # --- Grader ì„¤ì • ---
    relevance_system = """You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \n Provide the binary score as a JSON with a single key 'score' and no premable or explanation."""
    relevance_prompt = ChatPromptTemplate.from_messages(
        [("system", relevance_system), ("human", "question: {question}\n\n document: {document} ")])
    retrieval_grader = relevance_prompt | llm | JsonOutputParser()

    hallucination_system = """You are a grader assessing whether an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation."""
    hallucination_prompt = ChatPromptTemplate.from_messages(
        [("system", hallucination_system), ("human", "documents: {documents}\n\n answer: {generation} ")])
    hallucination_grader = hallucination_prompt | llm | JsonOutputParser()

    # --- ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---
    def retrieve_docs(state: GraphState) -> dict:
        st.session_state.logs.append("---NODE: Docs Retrieval---")
        question = state['question']
        documents = retriever.invoke(question)
        return {"documents": documents, "search_attempts": 0, "regeneration_attempts": 0, "error": None}

    def relevance_checker(state: GraphState) -> dict:
        st.session_state.logs.append("---NODE: Relevance Checker---")
        question = state['question']
        documents = state['documents']
        if not documents:
            st.session_state.logs.append("  -> ê´€ë ¨ì„± ê²€ì‚¬ ê²°ê³¼: ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ")
            return {"is_relevant": False}
        is_relevant = False
        for d in documents:
            doc_txt = d.page_content
            score = retrieval_grader.invoke({"question": question, "document": doc_txt})
            grade = score['score']
            if grade == 'yes':
                st.session_state.logs.append(f"  -> ê´€ë ¨ì„± ê²€ì‚¬ ê²°ê³¼: ê´€ë ¨ ë¬¸ì„œ ë°œê²¬ (score: {grade})")
                is_relevant = True
                break
            else:
                st.session_state.logs.append(f"  -> ê´€ë ¨ì„± ê²€ì‚¬ ê²°ê³¼: ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ (score: {grade})")
        return {"is_relevant": is_relevant}

    def generate_answer(state: GraphState) -> dict:
        st.session_state.logs.append("---NODE: Generate Answer---")
        question = state['question']
        documents = state['documents']
        regeneration_attempts = state['regeneration_attempts']
        system = """You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise"""
        prompt = ChatPromptTemplate.from_messages(
            [("system", system), ("human", "question: {question}\n\n context: {context} ")])
        rag_chain = prompt | llm | StrOutputParser()
        generation = rag_chain.invoke({"context": documents, "question": question})
        return {"generation": generation, "regeneration_attempts": regeneration_attempts + 1}

    def hallucination_checker(state: GraphState) -> dict:
        st.session_state.logs.append("---NODE: Hallucination Checker---")
        documents = state['documents']
        generation = state['generation']
        score = hallucination_grader.invoke({"documents": documents, "generation": generation})
        grade = score['score']
        if grade == 'yes':
            st.session_state.logs.append("  -> í™˜ê° ê²€ì‚¬ ê²°ê³¼: ë‹µë³€ì´ ë¬¸ì„œì— ê¸°ë°˜í•¨ (score: yes)")
            is_hallucination = False
        else:
            st.session_state.logs.append("  -> í™˜ê° ê²€ì‚¬ ê²°ê³¼: ë‹µë³€ì´ ë¬¸ì„œì— ê¸°ë°˜í•˜ì§€ ì•ŠìŒ (score: no)")
            is_hallucination = True
        return {"is_hallucination": is_hallucination}

    def search_tavily(state: GraphState) -> dict:
        st.session_state.logs.append("---NODE: Search Tavily---")
        question = state['question']
        search_attempts = state['search_attempts']
        if not tavily_client:
            st.session_state.logs.append("  -> Tavily API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì›¹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {"documents": [], "search_attempts": search_attempts + 1}
        response = tavily_client.search(query=question, max_results=3)
        web_documents = [
            Document(page_content=obj["content"], metadata={"url": obj["url"], "title": obj.get("title", question)}) for
            obj in response['results']]
        return {"documents": web_documents, "search_attempts": search_attempts + 1}

    def prepare_final_answer(state: GraphState) -> dict:
        st.session_state.logs.append("---NODE: Prepare Final Answer---")
        generation = state['generation']
        documents = state['documents']
        sources = []
        for doc in documents:
            url = doc.metadata.get('url') or doc.metadata.get('source')
            title = doc.metadata.get('title', url)
            if url:
                sources.append({"title": str(title), "url": str(url)})
        unique_sources = [dict(t) for t in {tuple(d.items()) for d in sources}]
        return {"final_answer": generation, "final_sources": unique_sources}

    # --- ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜ ì •ì˜ ---
    def decide_after_relevance_check(state: GraphState) -> str:
        st.session_state.logs.append("---DECISION: After Relevance Check---")
        if state["is_relevant"]:
            st.session_state.logs.append("  -> ê²°ì •: ê´€ë ¨ì„± ìˆìŒ, ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì´ë™")
            return "generate"
        elif state["search_attempts"] < 1:
            st.session_state.logs.append("  -> ê²°ì •: ê´€ë ¨ì„± ì—†ìŒ, ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì´ë™")
            return "search"
        else:
            st.session_state.logs.append("  -> ê²°ì •: ì›¹ ê²€ìƒ‰ í›„ì—ë„ ê´€ë ¨ì„± ì—†ìŒ, ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ")
            return "end_not_relevant"

    def decide_after_hallucination_check(state: GraphState) -> str:
        st.session_state.logs.append("---DECISION: After Hallucination Check---")
        if not state["is_hallucination"]:
            st.session_state.logs.append("  -> ê²°ì •: í™˜ê° ì—†ìŒ, ìµœì¢… ë‹µë³€ ì¤€ë¹„ë¡œ ì´ë™")
            return "useful"
        elif state["regeneration_attempts"] < 2:
            st.session_state.logs.append("  -> ê²°ì •: í™˜ê° ìˆìŒ, ë‹µë³€ ì¬ì„±ì„±ìœ¼ë¡œ ì´ë™")
            return "regenerate"
        else:
            st.session_state.logs.append("  -> ê²°ì •: ì¬ì„±ì„± í›„ì—ë„ í™˜ê° ìˆìŒ, ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ")
            return "end_hallucination"

    # --- ì›Œí¬í”Œë¡œìš° êµ¬ì„± ---
    workflow = StateGraph(GraphState)
    workflow.add_node("retrieve", retrieve_docs)
    workflow.add_node("relevance_check", relevance_checker)
    workflow.add_node("generate", generate_answer)
    workflow.add_node("hallucination_check", hallucination_checker)
    workflow.add_node("search", search_tavily)
    workflow.add_node("prepare_final_answer", prepare_final_answer)
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "relevance_check")
    workflow.add_edge("search", "relevance_check")
    workflow.add_edge("generate", "hallucination_check")
    workflow.add_edge("prepare_final_answer", END)
    workflow.add_conditional_edges("relevance_check", decide_after_relevance_check,
                                   {"generate": "generate", "search": "search", "end_not_relevant": END})
    workflow.add_conditional_edges("hallucination_check", decide_after_hallucination_check,
                                   {"useful": "prepare_final_answer", "regenerate": "generate",
                                    "end_hallucination": END})

    return workflow.compile()


# --- 3. Streamlit UI ë° ì‹¤í–‰ ë¡œì§ ---

# VectorDB ì„¤ì •ì€ ë¦¬ì†ŒìŠ¤ ì†Œëª¨ê°€ í¬ë¯€ë¡œ ìºì‹±í•©ë‹ˆë‹¤.
@st.cache_resource
def setup_retriever():
    """ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  VectorDBë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    urls = [
        "https://lilianweng.github.io/posts/2023-06-23-agent/",
        "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
        "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
    ]
    docs = [WebBaseLoader(url).load() for url in urls]
    docs_list = [item for sublist in docs for item in sublist]
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)
    doc_splits = text_splitter.split_documents(docs_list)
    vectorstore = Chroma.from_documents(
        documents=doc_splits,
        collection_name="rag-chroma",
        embedding=OpenAIEmbeddings(model="text-embedding-3-small")
    )
    return vectorstore.as_retriever()


def main():
    st.set_page_config(page_title="LangGraph RAG Demo", page_icon="ğŸ”—")
    st.title("LangGraph RAG Demo")

    # ì‚¬ì´ë“œë°”ì— API í‚¤ ì…ë ¥ í•„ë“œ ì¶”ê°€
    with st.sidebar:
        st.header("API Keys")
        openai_api_key = st.text_input("OpenAI API Key", type="password")
        tavily_api_key = st.text_input("Tavily API Key", type="password")

    # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="What are the types of agent memory?")

    if st.button("ì‹¤í–‰"):
        if not openai_api_key or not tavily_api_key:
            st.error("ì‚¬ì´ë“œë°”ì— OpenAI ë° Tavily API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        # API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
        os.environ["OPENAI_API_KEY"] = openai_api_key
        os.environ["TAVILY_API_KEY"] = tavily_api_key

        # ë¡œê¹…ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.logs = []

        with st.spinner("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘..."):
            # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            retriever = setup_retriever()
            llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
            tavily_client = TavilyClient(api_key=tavily_api_key)

            # ì›Œí¬í”Œë¡œìš° ìƒì„±
            app = create_workflow(llm, retriever, tavily_client)

            # ë¡œê·¸ë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ
            log_container = st.expander("ì‹¤í–‰ ë¡œê·¸", expanded=True)
            log_placeholder = log_container.empty()

            inputs = {"question": question}
            final_state = None

            # ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ë° ë¡œê·¸ ì¶œë ¥
            for output in app.stream(inputs, {"recursion_limit": 15}):
                for key, value in output.items():
                    log_placeholder.text("\n".join(st.session_state.logs))
                final_state = list(output.values())[0]

            # ìµœì¢… ê²°ê³¼ í‘œì‹œ
            st.success("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ!")

            if final_state and final_state.get("final_answer"):
                st.subheader("âœ… ìµœì¢… ë‹µë³€")
                st.write(final_state["final_answer"])

                st.subheader("ğŸ“š ì¶œì²˜")
                for source in final_state["final_sources"]:
                    st.markdown(f"- [{source['title']}]({source['url']})")
            else:
                st.error("âŒ ì›Œí¬í”Œë¡œìš°ê°€ ë‹µë³€ ìƒì„± ì „ì— ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                if not final_state.get("is_relevant") and final_state.get("search_attempts", 0) > 0:
                    st.info("   ì´ìœ : ì›¹ ê²€ìƒ‰ í›„ì—ë„ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (failed: not relevant)")
                elif final_state.get("is_hallucination") and final_state.get("regeneration_attempts", 0) > 1:
                    st.info("   ì´ìœ : ë‹µë³€ ì¬ì„±ì„± í›„ì—ë„ í™˜ê°ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. (failed: hallucination)")


if __name__ == "__main__":
    main()